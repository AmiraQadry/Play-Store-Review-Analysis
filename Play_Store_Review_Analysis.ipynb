{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from textblob import TextBlob  # For sentiment analysis example"
      ],
      "metadata": {
        "id": "9nvIxW0akzxd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources if not already downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41gXANksk7Vf",
        "outputId": "3bc68be4-f6ac-4841-a6c2-329d76aba9c2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset (replace with your actual dataset path)\n",
        "df = pd.read_csv('/content/drive/MyDrive/reviews.csv')"
      ],
      "metadata": {
        "id": "sHbvZIUulYUc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for preprocessing text data\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and numbers\n",
        "    tokens = word_tokenize(text)  # Tokenization\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words and len(word) > 1]  # Remove stopwords\n",
        "    lemmatizer = WordNetLemmatizer()  # Lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "Xq9JEzKOlevf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to 'content' column\n",
        "df['clean_text'] = df['content'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "pafCdAC5ljof"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment analysis using TextBlob as an example\n",
        "def analyze_sentiment(text):\n",
        "    analysis = TextBlob(text)\n",
        "    sentiment = analysis.sentiment.polarity\n",
        "    if sentiment > 0:\n",
        "        return 'positive'\n",
        "    elif sentiment == 0:\n",
        "        return 'neutral'\n",
        "    else:\n",
        "        return 'negative'\n",
        "\n",
        "# Apply sentiment analysis\n",
        "df['predicted_sentiment'] = df['clean_text'].apply(analyze_sentiment)"
      ],
      "metadata": {
        "id": "SVolVgLaluFp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert ratings to sentiment categories for evaluation\n",
        "def convert_rating_to_sentiment(score):\n",
        "    if score >= 4:\n",
        "        return 'positive'\n",
        "    elif score == 3:\n",
        "        return 'neutral'\n",
        "    else:\n",
        "        return 'negative'\n",
        "\n",
        "# Apply conversion\n",
        "df['true_sentiment'] = df['score'].apply(convert_rating_to_sentiment)\n",
        "\n",
        "# Example Evaluation and Recommendations\n",
        "# Evaluation metrics example: Sentiment analysis accuracy\n",
        "accuracy = accuracy_score(df['true_sentiment'], df['predicted_sentiment'])\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(classification_report(df['true_sentiment'], df['predicted_sentiment']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1iQCxf3l_Bv",
        "outputId": "f34d16ae-7c4e-4132-8768-361637c28086"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5130852340936375\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.73      0.28      0.41      4850\n",
            "     neutral       0.15      0.19      0.17      1991\n",
            "    positive       0.57      0.82      0.68      5654\n",
            "\n",
            "    accuracy                           0.51     12495\n",
            "   macro avg       0.49      0.43      0.42     12495\n",
            "weighted avg       0.57      0.51      0.49     12495\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommendations based on analysis\n",
        "positive_reviews = df[df['predicted_sentiment'] == 'positive']\n",
        "common_issues = df[df['predicted_sentiment'] == 'negative']['clean_text'].value_counts().head(5)\n",
        "\n",
        "# Print recommendations\n",
        "print(\"Recommendations based on analysis:\")\n",
        "print(f\"Positive reviews count: {len(positive_reviews)}\")\n",
        "print(f\"Common negative issues:\")\n",
        "print(common_issues)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94LrkMJDmCIa",
        "outputId": "eaf30c8e-8d53-4738-af66-c14ed04248c8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommendations based on analysis:\n",
            "Positive reviews count: 8144\n",
            "Common negative issues:\n",
            "clean_text\n",
            "bad            26\n",
            "complicated    11\n",
            "worst app      10\n",
            "useless         8\n",
            "confusing       7\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Topic Modeling using LDA\n",
        "# Vectorize text data\n",
        "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "dtm = vectorizer.fit_transform(df['clean_text'])\n",
        "\n",
        "# Apply LDA\n",
        "lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "lda_model.fit(dtm)\n",
        "\n",
        "# Display top words per topic\n",
        "print(\"Top words per topic:\")\n",
        "for index, topic in enumerate(lda_model.components_):\n",
        "    print(f\"Topic {index + 1}:\")\n",
        "    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kcCyjQVmIHc",
        "outputId": "69e4c7ff-e1e1-4f7b-b51f-a43c3670c6c0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top words per topic:\n",
            "Topic 1:\n",
            "['day', 'need', 'work', 'use', 'sync', 'event', 'google', 'good', 'calendar', 'app']\n",
            "\n",
            "Topic 2:\n",
            "['new', 'pay', 'update', 'pro', 'feature', 'ad', 'free', 'premium', 'version', 'app']\n",
            "\n",
            "Topic 3:\n",
            "['reminder', 'dont', 'working', 'account', 'doesnt', 'phone', 'time', 'work', 'notification', 'app']\n",
            "\n",
            "Topic 4:\n",
            "['feature', 'like', 'reminder', 'option', 'time', 'day', 'add', 'app', 'list', 'task']\n",
            "\n",
            "Topic 5:\n",
            "['good', 'best', 'like', 'easy', 'really', 'love', 'habit', 'use', 'great', 'app']\n",
            "\n"
          ]
        }
      ]
    }
  ]
}